{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\", \"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your expected and actual outputs\n",
    "expected_output = \"\"\"The information listed at the right of the label tells you what to look for. At the top of the label, look for the serving size. \n",
    "                     The serving size tells you how much of the food you should eat to get the nutrients listed on the label. A cup of food from the \n",
    "                     label pictured below is a serving. The calories in one serving are listed next. In this food, there are 250 calories per serving. \n",
    "                     Final Answer : serving size\"\"\"\n",
    "\n",
    "before_fine_tuning = \"\"\"That's an easy one!\n",
    "\n",
    "                        The Daily Value (DV) tells you how much of the food you should eat to get the nutrients listed on the label!\n",
    "\n",
    "                        The DV is a standard reference amount used to determine the percentage of the nutrient in a serving size. It's a helpful guide to help you make informed choices about the foods you eat.\"\"\"\n",
    "\n",
    "after_fine_tuning = \"\"\"The answer is: The serving size tells you how much of the food you should eat to get the nutrients listed on the label.\n",
    "\n",
    "                        In other words, the serving size is the amount of food that provides the nutrient amounts listed on the label.\"\"\"\n",
    "\n",
    "HybridRAG = \"\"\"Serving size and number of servings per container are at the top of the label.\"\"\"\n",
    "\n",
    "HybridRAG_fine_tuned = \"\"\"According to the provided knowledge, the serving size and number of servings per container are at the top of the label. \n",
    "                          This information tells you how much of the food you should eat to get the nutrients listed on the label.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "EVALUATION_PROMPT_TEMPLATE = \"\"\"\n",
    "You will be given one actual output for the expected_output. Your task is to rate the actual output on one metric.\n",
    "Please make sure you read and understand these instructions very carefully. \n",
    "Please keep this expected output open while reviewing, and refer to it as needed.\n",
    "\n",
    "Evaluation Criteria:\n",
    "\n",
    "{criteria}\n",
    "\n",
    "Evaluation Steps:\n",
    "\n",
    "{steps}\n",
    "\n",
    "Example:\n",
    "\n",
    "Source Text:\n",
    "\n",
    "{expected_output}\n",
    "\n",
    "Actual Output:\n",
    "\n",
    "{actual_output}\n",
    "\n",
    "Evaluation Form (scores ONLY):\n",
    "\n",
    "- {metric_name}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# Metric 1: Relevance\n",
    "\n",
    "RELEVANCY_SCORE_CRITERIA = \"\"\"\n",
    "            Relevance(1-5) - selection of important content from the expected output. \\\n",
    "            The actual output should include only important information from the expected output. \\\n",
    "            Annotators were instructed to penalize expected output which contained redundancies and excess information.\n",
    "\"\"\"\n",
    "\n",
    "RELEVANCY_SCORE_STEPS = \"\"\"\n",
    "1. Read the summary and the source document carefully.\n",
    "2. Compare the summary to the source document and identify the main points of the article.\n",
    "3. Assess how well the summary covers the main points of the article, and how much irrelevant or redundant information it contains.\n",
    "4. Assign a relevance score from 1 to 5.\n",
    "\"\"\"\n",
    "\n",
    "# Metric 2: Coherence\n",
    "\n",
    "COHERENCE_SCORE_CRITERIA = \"\"\" Coherence - the collective quality of all sentences in the actual output based on the expected output\n",
    "\"\"\"\n",
    "\n",
    "COHERENCE_SCORE_STEPS = \"\"\"\n",
    "        1. Read the expected output carefully and identify the main topic and key points.,\n",
    "        2. Read the actual output and compare it to the expected output. Check if the actual output covers the main topic and key points of the expected output,and if it presents them in a clear and logical order.,\n",
    "        3. Assign a score for coherence on a scale of 1 to 5, where 1 is the lowest and 5 is the highest based on the Evaluation Criteria.\n",
    "\"\"\"\n",
    "\n",
    "# Metric 3: Consistency\n",
    "\n",
    "CORRECTNESS_SCORE_CRITERIA = \"\"\" Determine whether the actual output is factually correct based on the expected output.\n",
    "\"\"\"\n",
    "\n",
    "CORRECTNESS_SCORE_STEPS = \"\"\"\n",
    "       1. Read the actual output carefully,\n",
    "       2. Compare the actual output to the expected output and identify the main points of the expected out,\n",
    "       3. Assess how well the actual output the main points of the expected output, and how much irrelevant or redundant information it contains.,\n",
    "       4. Assign a relevance score from 1 to 5.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_c5082_row0_col2, #T_c5082_row1_col1, #T_c5082_row1_col2, #T_c5082_row2_col2 {\n",
       "  font-weight: bold;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_c5082\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"index_name level0\" >Summary Type</th>\n",
       "      <th id=\"T_c5082_level0_col0\" class=\"col_heading level0 col0\" >HybridRAG</th>\n",
       "      <th id=\"T_c5082_level0_col1\" class=\"col_heading level0 col1\" >HybridRAG_fine_tuned</th>\n",
       "      <th id=\"T_c5082_level0_col2\" class=\"col_heading level0 col2\" >after_fine_tuning</th>\n",
       "      <th id=\"T_c5082_level0_col3\" class=\"col_heading level0 col3\" >before_fine_tuning</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th class=\"index_name level0\" >Evaluation Type</th>\n",
       "      <th class=\"blank col0\" >&nbsp;</th>\n",
       "      <th class=\"blank col1\" >&nbsp;</th>\n",
       "      <th class=\"blank col2\" >&nbsp;</th>\n",
       "      <th class=\"blank col3\" >&nbsp;</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_c5082_level0_row0\" class=\"row_heading level0 row0\" >Coherence</th>\n",
       "      <td id=\"T_c5082_row0_col0\" class=\"data row0 col0\" >4</td>\n",
       "      <td id=\"T_c5082_row0_col1\" class=\"data row0 col1\" >4</td>\n",
       "      <td id=\"T_c5082_row0_col2\" class=\"data row0 col2\" >5</td>\n",
       "      <td id=\"T_c5082_row0_col3\" class=\"data row0 col3\" >1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_c5082_level0_row1\" class=\"row_heading level0 row1\" >Correctness</th>\n",
       "      <td id=\"T_c5082_row1_col0\" class=\"data row1 col0\" >4</td>\n",
       "      <td id=\"T_c5082_row1_col1\" class=\"data row1 col1\" >5</td>\n",
       "      <td id=\"T_c5082_row1_col2\" class=\"data row1 col2\" >5</td>\n",
       "      <td id=\"T_c5082_row1_col3\" class=\"data row1 col3\" >1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_c5082_level0_row2\" class=\"row_heading level0 row2\" >Relevance</th>\n",
       "      <td id=\"T_c5082_row2_col0\" class=\"data row2 col0\" >4</td>\n",
       "      <td id=\"T_c5082_row2_col1\" class=\"data row2 col1\" >4</td>\n",
       "      <td id=\"T_c5082_row2_col2\" class=\"data row2 col2\" >5</td>\n",
       "      <td id=\"T_c5082_row2_col3\" class=\"data row2 col3\" >1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x1a41ed77210>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def highlight_max(s):\n",
    "    is_max = s == s.max()\n",
    "    return [\n",
    "        \"font-weight: bold\" if v else \"\"  # No background, just bold the max value\n",
    "        for v in is_max\n",
    "    ]\n",
    "\n",
    "\n",
    "def get_geval_score(\n",
    "    criteria: str, steps: str, expected_output: str, actual_output: str, metric_name: str\n",
    "):\n",
    "    prompt = EVALUATION_PROMPT_TEMPLATE.format(\n",
    "        criteria=criteria,\n",
    "        steps=steps,\n",
    "        expected_output=expected_output,  # Correct placeholder\n",
    "        actual_output=actual_output,      # Correct placeholder\n",
    "        metric_name=metric_name,\n",
    "    )\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=0,\n",
    "        max_tokens=5,\n",
    "        top_p=1,\n",
    "        frequency_penalty=0,\n",
    "        presence_penalty=0,\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "\n",
    "evaluation_metrics = {\n",
    "    \"Relevance\": (RELEVANCY_SCORE_CRITERIA, RELEVANCY_SCORE_STEPS),\n",
    "    \"Coherence\": (COHERENCE_SCORE_CRITERIA, COHERENCE_SCORE_STEPS),\n",
    "    \"Correctness\": (CORRECTNESS_SCORE_CRITERIA, CORRECTNESS_SCORE_STEPS),\n",
    "}\n",
    "\n",
    "summaries = {\"before_fine_tuning\": before_fine_tuning, \"after_fine_tuning\": after_fine_tuning , \"HybridRAG\":HybridRAG , \"HybridRAG_fine_tuned\":HybridRAG_fine_tuned}\n",
    "\n",
    "data = {\"Evaluation Type\": [], \"Summary Type\": [], \"Score\": []}\n",
    "\n",
    "for eval_type, (criteria, steps) in evaluation_metrics.items():\n",
    "    for summ_type, summary in summaries.items():\n",
    "        data[\"Evaluation Type\"].append(eval_type)\n",
    "        data[\"Summary Type\"].append(summ_type)\n",
    "        result = get_geval_score(criteria, steps, expected_output, summary, eval_type)\n",
    "        score_num = int(result.strip())\n",
    "        data[\"Score\"].append(score_num)\n",
    "\n",
    "pivot_df = pd.DataFrame(data, index=None).pivot(\n",
    "    index=\"Evaluation Type\", columns=\"Summary Type\", values=\"Score\"\n",
    ")\n",
    "styled_pivot_df = pivot_df.style.apply(highlight_max, axis=1)\n",
    "display(styled_pivot_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mrtar\\Desktop\\fine_tuning_model\\myenv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before_fine_tuning F1 Score: 0.9150149822235107\n",
      "after_fine_tuning 2 F1 Score: 0.9227251410484314\n",
      "HybridRAG F1 Score: 0.8197610974311829\n",
      "HybridRAG_fine_tuned F1 Score: 0.8484337329864502\n",
      "before_fine_tuning Precision: 0.9165672063827515\n",
      "after_fine_tuning  Precision: 0.9369108080863953\n",
      "HybridRAG Precision: 0.8972974419593811\n",
      "HybridRAG_fine_tuned Precision: 0.9323158860206604\n",
      "before_fine_tuning Recall: 0.9134680032730103\n",
      "after_fine_tuning 2 Recall: 0.9089627265930176\n",
      "HybridRAG Recall: 0.7545589804649353\n",
      "HybridRAG_fine_tuned Recall: 0.778399646282196\n"
     ]
    }
   ],
   "source": [
    "from bert_score import BERTScorer\n",
    "# Instantiate the BERTScorer object for English language\n",
    "scorer = BERTScorer(lang=\"en\")\n",
    "\n",
    "# Calculate BERTScore for the summary 1 against the excerpt\n",
    "# P1, R1, F1_1 represent Precision, Recall, and F1 Score respectively\n",
    "P1, R1, F1_1 = scorer.score([before_fine_tuning], [expected_output])\n",
    "\n",
    "# Calculate BERTScore for summary 2 against the excerpt\n",
    "# P2, R2, F2_2 represent Precision, Recall, and F1 Score respectively\n",
    "P2, R2, F2_2 = scorer.score([after_fine_tuning], [expected_output])\n",
    "\n",
    "P3, R3, F2_3 = scorer.score([HybridRAG], [expected_output])\n",
    "\n",
    "P4, R4, F2_4 = scorer.score([HybridRAG_fine_tuned], [expected_output])\n",
    "\n",
    "print(\"before_fine_tuning F1 Score:\", F1_1.tolist()[0])\n",
    "print(\"after_fine_tuning 2 F1 Score:\", F2_2.tolist()[0])\n",
    "print(\"HybridRAG F1 Score:\", F2_3.tolist()[0])\n",
    "print(\"HybridRAG_fine_tuned F1 Score:\", F2_4.tolist()[0])\n",
    "print(\"before_fine_tuning Precision:\", P1.tolist()[0])\n",
    "print(\"after_fine_tuning  Precision:\", P2.tolist()[0])\n",
    "print(\"HybridRAG Precision:\", P3.tolist()[0])\n",
    "print(\"HybridRAG_fine_tuned Precision:\", P4.tolist()[0])\n",
    "print(\"before_fine_tuning Recall:\", R1.tolist()[0])\n",
    "print(\"after_fine_tuning 2 Recall:\", R2.tolist()[0])\n",
    "print(\"HybridRAG Recall:\", R3.tolist()[0])\n",
    "print(\"HybridRAG_fine_tuned Recall:\", R4.tolist()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
