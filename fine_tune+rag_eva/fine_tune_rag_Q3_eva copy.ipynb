{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\", \"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your expected and actual outputs\n",
    "expected_output = \"\"\"The crocodilian digestive system is highly adapted to their lifestyle. Crocodilians are known to swallow stones, known as gastroliths , \n",
    "                     which help digest their prey. The crocodilian stomach is divided into two chambers. The first is powerful and muscular. \n",
    "                     The other stomach is the most acidic digestive system of any animal. It can digest mostly everything from their prey, including bones, \n",
    "                     feathers, and horns!. Final answer : two\"\"\"\n",
    "\n",
    "before_fine_tuning = \"\"\"The correct answer is: None!\n",
    "                        Crocodiles, like all reptiles, have a simple, tubular stomach without any chambers. The stomach is a long, \n",
    "                        muscular tube that helps to mix and break down food, but it does not have separate chambers like the stomachs \n",
    "                        of mammals and some other animals.\"\"\"\n",
    "\n",
    "after_fine_tuning = \"\"\"That's a clever question! However, I must clarify that crocodiles do not have stomachs in the classical sense. Instead, \n",
    "                       they have a digestive system that involves breaking down food in their mouth and then absorbing nutrients through their skin.\n",
    "                       So, to answer your question, the stomach of a crocodile does not have any chambers.\"\"\"\n",
    "\n",
    "HybridRAG = \"\"\"two_chambers\"\"\"\n",
    "\n",
    "HybridRAG_fine_tuned = \"\"\"Answer: Based on the provided knowledge, I can answer that the stomach of a crocodile has two chambers.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "EVALUATION_PROMPT_TEMPLATE = \"\"\"\n",
    "You will be given one actual output for the expected_output. Your task is to rate the actual output on one metric.\n",
    "Please make sure you read and understand these instructions very carefully. \n",
    "Please keep this expected output open while reviewing, and refer to it as needed.\n",
    "\n",
    "Evaluation Criteria:\n",
    "\n",
    "{criteria}\n",
    "\n",
    "Evaluation Steps:\n",
    "\n",
    "{steps}\n",
    "\n",
    "Example:\n",
    "\n",
    "Source Text:\n",
    "\n",
    "{expected_output}\n",
    "\n",
    "Actual Output:\n",
    "\n",
    "{actual_output}\n",
    "\n",
    "Evaluation Form (scores ONLY):\n",
    "\n",
    "- {metric_name}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# Metric 1: Relevance\n",
    "\n",
    "RELEVANCY_SCORE_CRITERIA = \"\"\"\n",
    "            Relevance(1-5) - selection of important content from the expected output. \\\n",
    "            The actual output should include only important information from the expected output. \\\n",
    "            Annotators were instructed to penalize expected output which contained redundancies and excess information.\n",
    "\"\"\"\n",
    "\n",
    "RELEVANCY_SCORE_STEPS = \"\"\"\n",
    "1. Read the summary and the source document carefully.\n",
    "2. Compare the summary to the source document and identify the main points of the article.\n",
    "3. Assess how well the summary covers the main points of the article, and how much irrelevant or redundant information it contains.\n",
    "4. Assign a relevance score from 1 to 5.\n",
    "\"\"\"\n",
    "\n",
    "# Metric 2: Coherence\n",
    "\n",
    "COHERENCE_SCORE_CRITERIA = \"\"\" Coherence - the collective quality of all sentences in the actual output based on the expected output\n",
    "\"\"\"\n",
    "\n",
    "COHERENCE_SCORE_STEPS = \"\"\"\n",
    "        1. Read the expected output carefully and identify the main topic and key points.,\n",
    "        2. Read the actual output and compare it to the expected output. Check if the actual output covers the main topic and key points of the expected output,and if it presents them in a clear and logical order.,\n",
    "        3. Assign a score for coherence on a scale of 1 to 5, where 1 is the lowest and 5 is the highest based on the Evaluation Criteria.\n",
    "\"\"\"\n",
    "\n",
    "# Metric 3: Consistency\n",
    "\n",
    "CORRECTNESS_SCORE_CRITERIA = \"\"\" Determine whether the actual output is factually correct based on the expected output.\n",
    "\"\"\"\n",
    "\n",
    "CORRECTNESS_SCORE_STEPS = \"\"\"\n",
    "       1. Read the actual output carefully,\n",
    "       2. Compare the actual output to the expected output and identify the main points of the expected out,\n",
    "       3. Assess how well the actual output the main points of the expected output, and how much irrelevant or redundant information it contains.,\n",
    "       4. Assign a relevance score from 1 to 5.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_f10ba_row0_col0, #T_f10ba_row0_col1, #T_f10ba_row1_col0, #T_f10ba_row1_col1, #T_f10ba_row2_col0, #T_f10ba_row2_col1 {\n",
       "  font-weight: bold;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_f10ba\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"index_name level0\" >Summary Type</th>\n",
       "      <th id=\"T_f10ba_level0_col0\" class=\"col_heading level0 col0\" >HybridRAG</th>\n",
       "      <th id=\"T_f10ba_level0_col1\" class=\"col_heading level0 col1\" >HybridRAG_fine_tuned</th>\n",
       "      <th id=\"T_f10ba_level0_col2\" class=\"col_heading level0 col2\" >after_fine_tuning</th>\n",
       "      <th id=\"T_f10ba_level0_col3\" class=\"col_heading level0 col3\" >before_fine_tuning</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th class=\"index_name level0\" >Evaluation Type</th>\n",
       "      <th class=\"blank col0\" >&nbsp;</th>\n",
       "      <th class=\"blank col1\" >&nbsp;</th>\n",
       "      <th class=\"blank col2\" >&nbsp;</th>\n",
       "      <th class=\"blank col3\" >&nbsp;</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_f10ba_level0_row0\" class=\"row_heading level0 row0\" >Coherence</th>\n",
       "      <td id=\"T_f10ba_row0_col0\" class=\"data row0 col0\" >5</td>\n",
       "      <td id=\"T_f10ba_row0_col1\" class=\"data row0 col1\" >5</td>\n",
       "      <td id=\"T_f10ba_row0_col2\" class=\"data row0 col2\" >1</td>\n",
       "      <td id=\"T_f10ba_row0_col3\" class=\"data row0 col3\" >1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f10ba_level0_row1\" class=\"row_heading level0 row1\" >Correctness</th>\n",
       "      <td id=\"T_f10ba_row1_col0\" class=\"data row1 col0\" >5</td>\n",
       "      <td id=\"T_f10ba_row1_col1\" class=\"data row1 col1\" >5</td>\n",
       "      <td id=\"T_f10ba_row1_col2\" class=\"data row1 col2\" >1</td>\n",
       "      <td id=\"T_f10ba_row1_col3\" class=\"data row1 col3\" >1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f10ba_level0_row2\" class=\"row_heading level0 row2\" >Relevance</th>\n",
       "      <td id=\"T_f10ba_row2_col0\" class=\"data row2 col0\" >5</td>\n",
       "      <td id=\"T_f10ba_row2_col1\" class=\"data row2 col1\" >5</td>\n",
       "      <td id=\"T_f10ba_row2_col2\" class=\"data row2 col2\" >1</td>\n",
       "      <td id=\"T_f10ba_row2_col3\" class=\"data row2 col3\" >1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x19cd1ccc210>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def highlight_max(s):\n",
    "    is_max = s == s.max()\n",
    "    return [\n",
    "        \"font-weight: bold\" if v else \"\"  # No background, just bold the max value\n",
    "        for v in is_max\n",
    "    ]\n",
    "\n",
    "\n",
    "def get_geval_score(\n",
    "    criteria: str, steps: str, expected_output: str, actual_output: str, metric_name: str\n",
    "):\n",
    "    prompt = EVALUATION_PROMPT_TEMPLATE.format(\n",
    "        criteria=criteria,\n",
    "        steps=steps,\n",
    "        expected_output=expected_output,  # Correct placeholder\n",
    "        actual_output=actual_output,      # Correct placeholder\n",
    "        metric_name=metric_name,\n",
    "    )\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=0,\n",
    "        max_tokens=5,\n",
    "        top_p=1,\n",
    "        frequency_penalty=0,\n",
    "        presence_penalty=0,\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "\n",
    "evaluation_metrics = {\n",
    "    \"Relevance\": (RELEVANCY_SCORE_CRITERIA, RELEVANCY_SCORE_STEPS),\n",
    "    \"Coherence\": (COHERENCE_SCORE_CRITERIA, COHERENCE_SCORE_STEPS),\n",
    "    \"Correctness\": (CORRECTNESS_SCORE_CRITERIA, CORRECTNESS_SCORE_STEPS),\n",
    "}\n",
    "\n",
    "summaries = {\"before_fine_tuning\": before_fine_tuning, \"after_fine_tuning\": after_fine_tuning , \"HybridRAG\":HybridRAG , \"HybridRAG_fine_tuned\":HybridRAG_fine_tuned}\n",
    "\n",
    "data = {\"Evaluation Type\": [], \"Summary Type\": [], \"Score\": []}\n",
    "\n",
    "for eval_type, (criteria, steps) in evaluation_metrics.items():\n",
    "    for summ_type, summary in summaries.items():\n",
    "        data[\"Evaluation Type\"].append(eval_type)\n",
    "        data[\"Summary Type\"].append(summ_type)\n",
    "        result = get_geval_score(criteria, steps, expected_output, summary, eval_type)\n",
    "        score_num = int(result.strip())\n",
    "        data[\"Score\"].append(score_num)\n",
    "\n",
    "pivot_df = pd.DataFrame(data, index=None).pivot(\n",
    "    index=\"Evaluation Type\", columns=\"Summary Type\", values=\"Score\"\n",
    ")\n",
    "styled_pivot_df = pivot_df.style.apply(highlight_max, axis=1)\n",
    "display(styled_pivot_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mrtar\\Desktop\\fine_tuning_model\\myenv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before_fine_tuning F1 Score: 0.7981292009353638\n",
      "after_fine_tuning 2 F1 Score: 0.814254105091095\n",
      "HybridRAG F1 Score: 0.7817310094833374\n",
      "HybridRAG_fine_tuned F1 Score: 0.8686626553535461\n",
      "before_fine_tuning Precision: 0.7413856983184814\n",
      "after_fine_tuning  Precision: 0.7742841839790344\n",
      "HybridRAG Precision: 0.7801142930984497\n",
      "HybridRAG_fine_tuned Precision: 0.8831270933151245\n",
      "before_fine_tuning Recall: 0.8642784357070923\n",
      "after_fine_tuning 2 Recall: 0.8585754036903381\n",
      "HybridRAG Recall: 0.7833545207977295\n",
      "HybridRAG_fine_tuned Recall: 0.8546644449234009\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from bert_score import BERTScorer\n",
    "# Instantiate the BERTScorer object for English language\n",
    "scorer = BERTScorer(lang=\"en\")\n",
    "\n",
    "# Calculate BERTScore for the summary 1 against the excerpt\n",
    "# P1, R1, F1_1 represent Precision, Recall, and F1 Score respectively\n",
    "P1, R1, F1_1 = scorer.score([before_fine_tuning], [expected_output])\n",
    "\n",
    "# Calculate BERTScore for summary 2 against the excerpt\n",
    "# P2, R2, F2_2 represent Precision, Recall, and F1 Score respectively\n",
    "P2, R2, F2_2 = scorer.score([after_fine_tuning], [expected_output])\n",
    "\n",
    "P3, R3, F2_3 = scorer.score([HybridRAG], [expected_output])\n",
    "\n",
    "P4, R4, F2_4 = scorer.score([HybridRAG_fine_tuned], [expected_output])\n",
    "\n",
    "print(\"before_fine_tuning F1 Score:\", F1_1.tolist()[0])\n",
    "print(\"after_fine_tuning 2 F1 Score:\", F2_2.tolist()[0])\n",
    "print(\"HybridRAG F1 Score:\", F2_3.tolist()[0])\n",
    "print(\"HybridRAG_fine_tuned F1 Score:\", F2_4.tolist()[0])\n",
    "print(\"before_fine_tuning Precision:\", P1.tolist()[0])\n",
    "print(\"after_fine_tuning  Precision:\", P2.tolist()[0])\n",
    "print(\"HybridRAG Precision:\", P3.tolist()[0])\n",
    "print(\"HybridRAG_fine_tuned Precision:\", P4.tolist()[0])\n",
    "print(\"before_fine_tuning Recall:\", R1.tolist()[0])\n",
    "print(\"after_fine_tuning 2 Recall:\", R2.tolist()[0])\n",
    "print(\"HybridRAG Recall:\", R3.tolist()[0])\n",
    "print(\"HybridRAG_fine_tuned Recall:\", R4.tolist()[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
