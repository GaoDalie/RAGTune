{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\", \"YOUR_API_KEY_HERE\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your expected and actual outputs\n",
    "expected_output = \"\"\"42.1 Circulatory systems link exchange surfaces with cells throughout the body. Final Answer :circulatory\"\"\"\n",
    "\n",
    "before_fine_tuning = \"\"\"The organ systems that link exchange surfaces with cells throughout the body are:\n",
    "                    Vascular System (Circulatory System): Consists of the heart, arteries, veins, and blood vessels that transport oxygenated blood to body tissues and coordinate the exchange of oxygen, carbon dioxide, and nutrient molecules between the blood and cells.\n",
    "                    Respiratory System: Comprises the lungs, trachea, and bronchi that facilitate the exchange of oxygen and carbon dioxide through inhalation and exhalation, allowing cells to extract oxygen and eliminate carbon dioxide.\n",
    "                    Urinary System (Excretory System): Includes the kidneys, ureters, bladder, and urethra that filter and eliminate waste products, excess ions, and water from the body, maintaining a stable environment for cells.\n",
    "                    Integumentary System: Encompasses the skin, hair, nails, and associated glands that regulate body temperature, protect against external damage, and facilitate the exchange of materials.\"\"\"\n",
    "\n",
    "after_fine_tuning = \"\"\"A. circulatory system\n",
    "                    B. respiratory system\n",
    "                    C. skeletal system\n",
    "                    D. integumentary system\n",
    "\n",
    "                    Answer: A. circulatory system\n",
    "                    The circulatory system, also known as the cardiovascular system, is responsible for moving blood throughout the body. It links exchange surfaces with cells throughout the body. The main organs of the circulatory system are the heart, arteries, veins, and blood vessels. The heart pumps blood throughout the body. Arteries carry oxygenated blood away from the heart. Veins return deoxygenated blood to the heart. Blood vessels include both arteries and veins. The circulatory system is responsible for delivering oxygen and nutrients to cells and organs throughout the body. It also picks up waste products and returns them to the excretory system for elimination. In addition, the circulatory system plays a key role in maintaining body temperature, as it helps to distribute heat generated by metabolic processes throughout the body.\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define your expected and actual outputs\n",
    "# expected_output = \"\"\"The metals of group 13 (Al, Ga, In, and Tl) are all reactive. However, \n",
    "#                     passivation occurs as a tough, hard, thin film of the metal oxide forms \n",
    "#                     upon exposure to air. Disruption of this film may counter the passivation, \n",
    "#                     allowing the metal to react. One way to disrupt the film is to expose the \n",
    "#                     passivated metal to mercury. Some of the metal dissolves in the mercury to \n",
    "#                     form an amalgam, which sheds the protective oxide layer to expose the metal \n",
    "#                     to further reaction. The formation of an amalgam allows the metal to react \n",
    "#                     with air and water. Final answer : air and water\"\"\"\n",
    "\n",
    "# before_fine_tuning = \"\"\"The correct answer is: mercury.\n",
    "#                     When a metal reacts with mercury, it forms an amalgam. The reactants typically include a metal \n",
    "#                     (such as silver, gold, or tin) and mercury. During this reaction, the metal atoms lose electrons\n",
    "#                     and form ions, which then combine with mercury atoms to form a new compound with a distinct color,\n",
    "#                     texture, and properties.\"\"\"\n",
    "\n",
    "# after_fine_tuning = \"\"\"A. Air\n",
    "#                     B. Water\n",
    "#                     C. Mercury\n",
    "#                     D. The surrounding tooth material\n",
    "                    \n",
    "#                     Answer: D. The surrounding tooth material\n",
    "#                     The formation of an amalgam allows the metal to react with the surrounding tooth material, which is \n",
    "#                     mostly dentin. The mercury in the amalgam reacts with the copper in the alloy to form a stable compound \n",
    "#                     that prevents further corrosion. The resulting amalgam is strong and durable, making it well suited for \n",
    "#                     use in dental restorations such as fillings and inlays. In addition, amalgams are also used in crowns, \n",
    "#                     bridges, and dentures.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "EVALUATION_PROMPT_TEMPLATE = \"\"\"\n",
    "You will be given one actual output for the expected_output. Your task is to rate the actual output on one metric.\n",
    "Please make sure you read and understand these instructions very carefully. \n",
    "Please keep this expected output open while reviewing, and refer to it as needed.\n",
    "\n",
    "Evaluation Criteria:\n",
    "\n",
    "{criteria}\n",
    "\n",
    "Evaluation Steps:\n",
    "\n",
    "{steps}\n",
    "\n",
    "Example:\n",
    "\n",
    "Source Text:\n",
    "\n",
    "{expected_output}\n",
    "\n",
    "Actual Output:\n",
    "\n",
    "{actual_output}\n",
    "\n",
    "Evaluation Form (scores ONLY):\n",
    "\n",
    "- {metric_name}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# Metric 1: Relevance\n",
    "\n",
    "RELEVANCY_SCORE_CRITERIA = \"\"\"\n",
    "            Relevance(1-5) - selection of important content from the expected output. \\\n",
    "            The actual output should include only important information from the expected output. \\\n",
    "            Annotators were instructed to penalize expected output which contained redundancies and excess information.\n",
    "\"\"\"\n",
    "\n",
    "RELEVANCY_SCORE_STEPS = \"\"\"\n",
    "1. Read the summary and the source document carefully.\n",
    "2. Compare the summary to the source document and identify the main points of the article.\n",
    "3. Assess how well the summary covers the main points of the article, and how much irrelevant or redundant information it contains.\n",
    "4. Assign a relevance score from 1 to 5.\n",
    "\"\"\"\n",
    "\n",
    "# Metric 2: Coherence\n",
    "\n",
    "COHERENCE_SCORE_CRITERIA = \"\"\" Coherence - the collective quality of all sentences in the actual output based on the expected output\n",
    "\"\"\"\n",
    "\n",
    "COHERENCE_SCORE_STEPS = \"\"\"\n",
    "        1. Read the expected output carefully and identify the main topic and key points.,\n",
    "        2. Read the actual output and compare it to the expected output. Check if the actual output covers the main topic and key points of the expected output,and if it presents them in a clear and logical order.,\n",
    "        3. Assign a score for coherence on a scale of 1 to 5, where 1 is the lowest and 5 is the highest based on the Evaluation Criteria.\n",
    "\"\"\"\n",
    "\n",
    "# Metric 3: Consistency\n",
    "\n",
    "CORRECTNESS_SCORE_CRITERIA = \"\"\" Determine whether the actual output is factually correct based on the expected output.\n",
    "\"\"\"\n",
    "\n",
    "CORRECTNESS_SCORE_STEPS = \"\"\"\n",
    "       1. Read the actual output carefully,\n",
    "       2. Compare the actual output to the expected output and identify the main points of the expected out,\n",
    "       3. Assess how well the actual output the main points of the expected output, and how much irrelevant or redundant information it contains.,\n",
    "       4. Assign a relevance score from 1 to 5.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_0c91b_row0_col0, #T_0c91b_row0_col1, #T_0c91b_row1_col0, #T_0c91b_row1_col1, #T_0c91b_row2_col0 {\n",
       "  font-weight: bold;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_0c91b\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"index_name level0\" >Summary Type</th>\n",
       "      <th id=\"T_0c91b_level0_col0\" class=\"col_heading level0 col0\" >after_fine_tuning</th>\n",
       "      <th id=\"T_0c91b_level0_col1\" class=\"col_heading level0 col1\" >before_fine_tuning</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th class=\"index_name level0\" >Evaluation Type</th>\n",
       "      <th class=\"blank col0\" >&nbsp;</th>\n",
       "      <th class=\"blank col1\" >&nbsp;</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_0c91b_level0_row0\" class=\"row_heading level0 row0\" >Coherence</th>\n",
       "      <td id=\"T_0c91b_row0_col0\" class=\"data row0 col0\" >5</td>\n",
       "      <td id=\"T_0c91b_row0_col1\" class=\"data row0 col1\" >5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_0c91b_level0_row1\" class=\"row_heading level0 row1\" >Correctness</th>\n",
       "      <td id=\"T_0c91b_row1_col0\" class=\"data row1 col0\" >5</td>\n",
       "      <td id=\"T_0c91b_row1_col1\" class=\"data row1 col1\" >5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_0c91b_level0_row2\" class=\"row_heading level0 row2\" >Relevance</th>\n",
       "      <td id=\"T_0c91b_row2_col0\" class=\"data row2 col0\" >5</td>\n",
       "      <td id=\"T_0c91b_row2_col1\" class=\"data row2 col1\" >2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x24e12d10410>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def highlight_max(s):\n",
    "    is_max = s == s.max()\n",
    "    return [\n",
    "        \"font-weight: bold\" if v else \"\"  # No background, just bold the max value\n",
    "        for v in is_max\n",
    "    ]\n",
    "\n",
    "\n",
    "def get_geval_score(\n",
    "    criteria: str, steps: str, expected_output: str, actual_output: str, metric_name: str\n",
    "):\n",
    "    prompt = EVALUATION_PROMPT_TEMPLATE.format(\n",
    "        criteria=criteria,\n",
    "        steps=steps,\n",
    "        expected_output=expected_output,  # Correct placeholder\n",
    "        actual_output=actual_output,      # Correct placeholder\n",
    "        metric_name=metric_name,\n",
    "    )\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=0,\n",
    "        max_tokens=5,\n",
    "        top_p=1,\n",
    "        frequency_penalty=0,\n",
    "        presence_penalty=0,\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "\n",
    "evaluation_metrics = {\n",
    "    \"Relevance\": (RELEVANCY_SCORE_CRITERIA, RELEVANCY_SCORE_STEPS),\n",
    "    \"Coherence\": (COHERENCE_SCORE_CRITERIA, COHERENCE_SCORE_STEPS),\n",
    "    \"Correctness\": (CORRECTNESS_SCORE_CRITERIA, CORRECTNESS_SCORE_STEPS),\n",
    "}\n",
    "\n",
    "summaries = {\"before_fine_tuning\": before_fine_tuning, \"after_fine_tuning\": after_fine_tuning}\n",
    "\n",
    "data = {\"Evaluation Type\": [], \"Summary Type\": [], \"Score\": []}\n",
    "\n",
    "for eval_type, (criteria, steps) in evaluation_metrics.items():\n",
    "    for summ_type, summary in summaries.items():\n",
    "        data[\"Evaluation Type\"].append(eval_type)\n",
    "        data[\"Summary Type\"].append(summ_type)\n",
    "        result = get_geval_score(criteria, steps, expected_output, summary, eval_type)\n",
    "        score_num = int(result.strip())\n",
    "        data[\"Score\"].append(score_num)\n",
    "\n",
    "pivot_df = pd.DataFrame(data, index=None).pivot(\n",
    "    index=\"Evaluation Type\", columns=\"Summary Type\", values=\"Score\"\n",
    ")\n",
    "styled_pivot_df = pivot_df.style.apply(highlight_max, axis=1)\n",
    "display(styled_pivot_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_dd80c_row0_col0, #T_dd80c_row1_col0, #T_dd80c_row2_col0 {\n",
       "  font-weight: bold;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_dd80c\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_dd80c_level0_col0\" class=\"col_heading level0 col0\" >before_fine_tuning</th>\n",
       "      <th id=\"T_dd80c_level0_col1\" class=\"col_heading level0 col1\" >after_fine_tuning</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th class=\"index_name level0\" >Metric</th>\n",
       "      <th class=\"blank col0\" >&nbsp;</th>\n",
       "      <th class=\"blank col1\" >&nbsp;</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_dd80c_level0_row0\" class=\"row_heading level0 row0\" >rouge-1 (F-Score)</th>\n",
       "      <td id=\"T_dd80c_row0_col0\" class=\"data row0 col0\" >0.180000</td>\n",
       "      <td id=\"T_dd80c_row0_col1\" class=\"data row0 col1\" >0.142857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_dd80c_level0_row1\" class=\"row_heading level0 row1\" >rouge-2 (F-Score)</th>\n",
       "      <td id=\"T_dd80c_row1_col0\" class=\"data row1 col0\" >0.101449</td>\n",
       "      <td id=\"T_dd80c_row1_col1\" class=\"data row1 col1\" >0.088235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_dd80c_level0_row2\" class=\"row_heading level0 row2\" >rouge-l (F-Score)</th>\n",
       "      <td id=\"T_dd80c_row2_col0\" class=\"data row2 col0\" >0.180000</td>\n",
       "      <td id=\"T_dd80c_row2_col1\" class=\"data row2 col1\" >0.142857</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x24e131dc150>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from rouge import Rouge\n",
    "import pandas as pd\n",
    "\n",
    "# function to calculate the Rouge score\n",
    "def get_rouge_scores(text1, text2):\n",
    "    rouge = Rouge()\n",
    "    return rouge.get_scores(text1, text2)\n",
    "\n",
    "\n",
    "# Calculate the ROUGE scores for both summaries using reference\n",
    "eval_1_rouge = get_rouge_scores(before_fine_tuning, expected_output)\n",
    "eval_2_rouge = get_rouge_scores(after_fine_tuning, expected_output)\n",
    "\n",
    "# Create a list to store the scores\n",
    "rouge_scores_out = []\n",
    "\n",
    "# Extract and store the scores\n",
    "for metric in [\"rouge-1\", \"rouge-2\", \"rouge-l\"]:\n",
    "    for label in [\"f\"]:\n",
    "        eval_1_score = eval_1_rouge[0][metric][label]\n",
    "        eval_2_score = eval_2_rouge[0][metric][label]\n",
    "\n",
    "        row = {\n",
    "            \"Metric\": f\"{metric} (F-Score)\",\n",
    "            \"before_fine_tuning\": eval_1_score,\n",
    "            \"after_fine_tuning\": eval_2_score,\n",
    "        }\n",
    "        rouge_scores_out.append(row)\n",
    "\n",
    "# Convert the results to a DataFrame and style it\n",
    "rouge_scores_out = pd.DataFrame(rouge_scores_out).set_index(\"Metric\")\n",
    "rouge_scores_out_styled = rouge_scores_out.style.apply(highlight_max, axis=1)\n",
    "\n",
    "rouge_scores_out_styled\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before_fine_tuning F1 Score: 0.8097435235977173\n",
      "after_fine_tuning 2 F1 Score: 0.8154077529907227\n",
      "before_fine_tuning Precision: 0.7407108545303345\n",
      "after_fine_tuning 2 Precision: 0.7454288005828857\n",
      "before_fine_tuning Recall: 0.8929659724235535\n",
      "after_fine_tuning 2 Recall: 0.8998866677284241\n"
     ]
    }
   ],
   "source": [
    "from bert_score import BERTScorer\n",
    "# Instantiate the BERTScorer object for English language\n",
    "scorer = BERTScorer(lang=\"en\")\n",
    "\n",
    "# Calculate BERTScore for the summary 1 agai\n",
    "# P1, R1, F1_1 represent Precision, Recall, and F1 Score respectively\n",
    "P1, R1, F1_1 = scorer.score([before_fine_tuning], [expected_output])\n",
    "\n",
    "# Calculate BERTScore for summary 2 against the excerpt\n",
    "# P2, R2, F2_2 represent Precision, Recall, and F1 Score respectively\n",
    "P2, R2, F2_2 = scorer.score([after_fine_tuning], [expected_output])\n",
    "\n",
    "print(\"before_fine_tuning F1 Score:\", F1_1.tolist()[0])\n",
    "print(\"after_fine_tuning 2 F1 Score:\", F2_2.tolist()[0])\n",
    "print(\"before_fine_tuning Precision:\", P1.tolist()[0])\n",
    "print(\"after_fine_tuning 2 Precision:\", P2.tolist()[0])\n",
    "print(\"before_fine_tuning Recall:\", R1.tolist()[0])\n",
    "print(\"after_fine_tuning 2 Recall:\", R2.tolist()[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
