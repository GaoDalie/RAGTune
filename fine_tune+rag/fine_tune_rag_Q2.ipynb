{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gpsXmlpzwqlj",
        "outputId": "d415634d-2c30-4f78-9242-ffa67bbe64e6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting llama-cpp-python\n",
            "  Downloading llama_cpp_python-0.3.1.tar.gz (63.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.9/63.9 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting langchain_community\n",
            "  Downloading langchain_community-0.3.3-py3-none-any.whl.metadata (2.8 kB)\n",
            "Collecting langchain_core\n",
            "  Downloading langchain_core-0.3.12-py3-none-any.whl.metadata (6.3 kB)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (0.24.7)\n",
            "Collecting pypdf\n",
            "  Downloading pypdf-5.0.1-py3-none-any.whl.metadata (7.4 kB)\n",
            "Collecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.9.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.4 kB)\n",
            "Collecting duckduckgo-search\n",
            "  Downloading duckduckgo_search-6.3.2-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting langchain-huggingface\n",
            "  Downloading langchain_huggingface-0.1.0-py3-none-any.whl.metadata (1.3 kB)\n",
            "Collecting google-search-results\n",
            "  Downloading google_search_results-2.4.2.tar.gz (18 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python) (4.12.2)\n",
            "Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python) (1.26.4)\n",
            "Collecting diskcache>=5.6.1 (from llama-cpp-python)\n",
            "  Downloading diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: jinja2>=2.11.3 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python) (3.1.4)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (2.0.36)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (3.10.10)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain_community)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting langchain<0.4.0,>=0.3.4 (from langchain_community)\n",
            "  Downloading langchain-0.3.4-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting langsmith<0.2.0,>=0.1.125 (from langchain_community)\n",
            "  Downloading langsmith-0.1.136-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain_community)\n",
            "  Downloading pydantic_settings-2.6.0-py3-none-any.whl.metadata (3.5 kB)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (2.32.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (9.0.0)\n",
            "Collecting jsonpatch<2.0,>=1.33 (from langchain_core)\n",
            "  Downloading jsonpatch-1.33-py2.py3-none-any.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain_core) (24.1)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.5.2 in /usr/local/lib/python3.10/dist-packages (from langchain_core) (2.9.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (3.16.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2024.6.1)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.66.5)\n",
            "Requirement already satisfied: click>=8.1.7 in /usr/local/lib/python3.10/dist-packages (from duckduckgo-search) (8.1.7)\n",
            "Collecting primp>=0.6.4 (from duckduckgo-search)\n",
            "  Downloading primp-0.6.4-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
            "Collecting sentence-transformers>=2.6.0 (from langchain-huggingface)\n",
            "  Downloading sentence_transformers-3.2.1-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: tokenizers>=0.19.1 in /usr/local/lib/python3.10/dist-packages (from langchain-huggingface) (0.19.1)\n",
            "Requirement already satisfied: transformers>=4.39.0 in /usr/local/lib/python3.10/dist-packages (from langchain-huggingface) (4.44.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.15.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (4.0.3)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading marshmallow-3.23.0-py3-none-any.whl.metadata (7.6 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2>=2.11.3->llama-cpp-python) (3.0.2)\n",
            "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain_core)\n",
            "  Downloading jsonpointer-3.0.0-py2.py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting langchain-text-splitters<0.4.0,>=0.3.0 (from langchain<0.4.0,>=0.3.4->langchain_community)\n",
            "  Downloading langchain_text_splitters-0.3.0-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting httpx<1,>=0.23.0 (from langsmith<0.2.0,>=0.1.125->langchain_community)\n",
            "  Downloading httpx-0.27.2-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting orjson<4.0.0,>=3.9.14 (from langsmith<0.2.0,>=0.1.125->langchain_community)\n",
            "  Downloading orjson-3.10.9-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.6/50.6 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting requests-toolbelt<2.0.0,>=1.0.0 (from langsmith<0.2.0,>=0.1.125->langchain_community)\n",
            "  Downloading requests_toolbelt-1.0.0-py2.py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain_core) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain_core) (2.23.4)\n",
            "Collecting python-dotenv>=0.21.0 (from pydantic-settings<3.0.0,>=2.4.0->langchain_community)\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain_community) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain_community) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain_community) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain_community) (2024.8.30)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=2.6.0->langchain-huggingface) (2.4.1+cu121)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=2.6.0->langchain-huggingface) (1.5.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=2.6.0->langchain-huggingface) (1.13.1)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=2.6.0->langchain-huggingface) (10.4.0)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain_community) (3.1.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.39.0->langchain-huggingface) (2024.9.11)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.39.0->langchain-huggingface) (0.4.5)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain_community) (3.7.1)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain_community)\n",
            "  Downloading httpcore-1.0.6-py3-none-any.whl.metadata (21 kB)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain_community) (1.3.1)\n",
            "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain_community)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface) (3.4.1)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp<4.0.0,>=3.8.3->langchain_community) (0.2.0)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers>=2.6.0->langchain-huggingface) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers>=2.6.0->langchain-huggingface) (3.5.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain_community) (1.2.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface) (1.3.0)\n",
            "Downloading langchain_community-0.3.3-py3-none-any.whl (2.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m57.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_core-0.3.12-py3-none-any.whl (407 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m407.7/407.7 kB\u001b[0m \u001b[31m22.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pypdf-5.0.1-py3-none-any.whl (294 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.5/294.5 kB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading faiss_cpu-1.9.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (27.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.5/27.5 MB\u001b[0m \u001b[31m43.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading duckduckgo_search-6.3.2-py3-none-any.whl (27 kB)\n",
            "Downloading langchain_huggingface-0.1.0-py3-none-any.whl (20 kB)\n",
            "Downloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
            "Downloading langchain-0.3.4-py3-none-any.whl (1.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m43.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langsmith-0.1.136-py3-none-any.whl (296 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m296.7/296.7 kB\u001b[0m \u001b[31m19.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading primp-0.6.4-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m61.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydantic_settings-2.6.0-py3-none-any.whl (28 kB)\n",
            "Downloading sentence_transformers-3.2.1-py3-none-any.whl (255 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m255.8/255.8 kB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpx-0.27.2-py3-none-any.whl (76 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpcore-1.0.6-py3-none-any.whl (78 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.0/78.0 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jsonpointer-3.0.0-py2.py3-none-any.whl (7.6 kB)\n",
            "Downloading langchain_text_splitters-0.3.0-py3-none-any.whl (25 kB)\n",
            "Downloading marshmallow-3.23.0-py3-none-any.whl (49 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.5/49.5 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading orjson-3.10.9-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (144 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m144.5/144.5 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Downloading requests_toolbelt-1.0.0-py2.py3-none-any.whl (54 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.5/54.5 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: llama-cpp-python, google-search-results\n",
            "  Building wheel for llama-cpp-python (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for llama-cpp-python: filename=llama_cpp_python-0.3.1-cp310-cp310-linux_x86_64.whl size=3485338 sha256=858da8bb25305f526e7b2d42514008c946e42b244f173cc24d586fb66eef7178\n",
            "  Stored in directory: /root/.cache/pip/wheels/f8/b0/a2/f47d952aec7ab061b9e2a345e23a1e1e137beb7891259e3d0c\n",
            "  Building wheel for google-search-results (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for google-search-results: filename=google_search_results-2.4.2-py3-none-any.whl size=32009 sha256=d4acd80dda953d3eae211646b9e256c7d9bd2c0344d87da2e777d10cd4d9e176\n",
            "  Stored in directory: /root/.cache/pip/wheels/d3/b2/c3/03302d12bb44a2cdff3c9371f31b72c0c4e84b8d2285eeac53\n",
            "Successfully built llama-cpp-python google-search-results\n",
            "Installing collected packages: python-dotenv, pypdf, primp, orjson, mypy-extensions, marshmallow, jsonpointer, h11, faiss-cpu, diskcache, typing-inspect, requests-toolbelt, llama-cpp-python, jsonpatch, httpcore, google-search-results, duckduckgo-search, pydantic-settings, httpx, dataclasses-json, langsmith, sentence-transformers, langchain_core, langchain-text-splitters, langchain-huggingface, langchain, langchain_community\n",
            "Successfully installed dataclasses-json-0.6.7 diskcache-5.6.3 duckduckgo-search-6.3.2 faiss-cpu-1.9.0 google-search-results-2.4.2 h11-0.14.0 httpcore-1.0.6 httpx-0.27.2 jsonpatch-1.33 jsonpointer-3.0.0 langchain-0.3.4 langchain-huggingface-0.1.0 langchain-text-splitters-0.3.0 langchain_community-0.3.3 langchain_core-0.3.12 langsmith-0.1.136 llama-cpp-python-0.3.1 marshmallow-3.23.0 mypy-extensions-1.0.0 orjson-3.10.9 primp-0.6.4 pydantic-settings-2.6.0 pypdf-5.0.1 python-dotenv-1.0.1 requests-toolbelt-1.0.0 sentence-transformers-3.2.1 typing-inspect-0.9.0\n"
          ]
        }
      ],
      "source": [
        "!pip install llama-cpp-python langchain_community langchain_core huggingface_hub pypdf faiss-cpu duckduckgo-search langchain-huggingface  google-search-results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190,
          "referenced_widgets": [
            "10a9fb6d877043d0b38d5afbce0bb2f9",
            "b60432cf141c472e96f954c08fe8eac0",
            "4a32671c2a364cef94a94fa4fcd3efab",
            "26ce532afb7f4416be1179f00e266372",
            "1ef3f5d94b5f440295a97358583cf8a6",
            "186e9cb83b11466b86d339e602df925d",
            "982afd866ab64066b41e3e2ae84ba55a",
            "bad5edf81fcc4490a7444a3589185a49",
            "725681421d554b8a9bea621c4e06ec92",
            "da9a87c403144c0485ccf8c254aeaf18",
            "b0020f10ba7d4346813f69fc74e9c696"
          ]
        },
        "id": "INBfzseIwqOM",
        "outputId": "1f89b834-9d20-42f7-99ae-6eb3a9b50ec0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "10a9fb6d877043d0b38d5afbce0bb2f9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "unsloth.Q4_K_M.gguf:   0%|          | 0.00/4.92G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "model download to: Models/unsloth.Q4_K_M.gguf\n"
          ]
        }
      ],
      "source": [
        "from huggingface_hub import hf_hub_download\n",
        "\n",
        "model_rpo = \"GaoDalie/llama-3-8b-Instruct-bnb-4bit-gaodalie-demo\"\n",
        "model_file = \"unsloth.Q4_K_M.gguf\"\n",
        "\n",
        "local_dir = \"Models\"\n",
        "\n",
        "download_file = hf_hub_download(repo_id=model_rpo,filename=model_file,local_dir=local_dir)\n",
        "\n",
        "print(f\"model download to: {download_file}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "JXkKbOOGzRcw"
      },
      "outputs": [],
      "source": [
        "Model_Path = 'Models/unsloth.Q4_K_M.gguf'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2TDz8IKHo0Ew",
        "outputId": "bbe67b8e-7446-41fe-8111-f91798a27888"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py:3553: LangChainDeprecationWarning: As of langchain-core 0.3.0, LangChain uses pydantic v2 internally. The langchain_core.pydantic_v1 module was a compatibility shim for pydantic v1, and should no longer be used. Please update the code to import from Pydantic directly.\n",
            "\n",
            "For example, replace imports like: `from langchain_core.pydantic_v1 import BaseModel`\n",
            "with: `from pydantic import BaseModel`\n",
            "or the v1 compatibility namespace if you are working in a code base that has not been fully upgraded to pydantic 2 yet. \tfrom pydantic.v1 import BaseModel\n",
            "\n",
            "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import sys\n",
        "from dotenv import load_dotenv\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain_core.pydantic_v1 import BaseModel, Field\n",
        "import json\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.llms import LlamaCpp\n",
        "from typing import List , Tuple\n",
        "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..'))) # Add the parent directory to the path sicnce we work with notebooks\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_community.tools import DuckDuckGoSearchResults\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "x-Z-2Ur9p_Dz"
      },
      "outputs": [],
      "source": [
        "path = \"/content/Lecture8.pdf\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "ef5f8OUyYBpQ"
      },
      "outputs": [],
      "source": [
        "embed_model = HuggingFaceEmbeddings(model_name=\"BAAI/bge-small-en-v1.5\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "OnkI7UJfRKt_"
      },
      "outputs": [],
      "source": [
        "def replace_t_with_space(list_of_documents):\n",
        "    \"\"\"\n",
        "    Replaces all tab characters ('\\t') with spaces in the page content of each document.\n",
        "\n",
        "    Args:\n",
        "        list_of_documents: A list of document objects, each with a 'page_content' attribute.\n",
        "\n",
        "    Returns:\n",
        "        The modified list of documents with tab characters replaced by spaces.\n",
        "    \"\"\"\n",
        "\n",
        "    for doc in list_of_documents:\n",
        "        doc.page_content = doc.page_content.replace('\\t', ' ')  # Replace tabs with spaces\n",
        "    return list_of_documents\n",
        "\n",
        "def encode_pdf(path, chunk_size=1000, chunk_overlap=200):\n",
        "    \"\"\"\n",
        "    Encodes a PDF book into a vector store using OpenAI embeddings.\n",
        "\n",
        "    Args:\n",
        "        path: The path to the PDF file.\n",
        "        chunk_size: The desired size of each text chunk.\n",
        "        chunk_overlap: The amount of overlap between consecutive chunks.\n",
        "\n",
        "    Returns:\n",
        "        A FAISS vector store containing the encoded book content.\n",
        "    \"\"\"\n",
        "\n",
        "    # Load PDF documents\n",
        "    loader = PyPDFLoader(path)\n",
        "    documents = loader.load()\n",
        "\n",
        "    # Split documents into chunks\n",
        "    text_splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=chunk_size, chunk_overlap=chunk_overlap, length_function=len\n",
        "    )\n",
        "    texts = text_splitter.split_documents(documents)\n",
        "    cleaned_texts = replace_t_with_space(texts)\n",
        "\n",
        "    # Create embeddings and vector store\n",
        "    embeddings = embed_model\n",
        "    vectorstore = FAISS.from_documents(cleaned_texts, embeddings)\n",
        "\n",
        "    return vectorstore"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "xS1oPxvnp_gg"
      },
      "outputs": [],
      "source": [
        "vectorstore = encode_pdf(path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rnq9lIseqBbs",
        "outputId": "4046402b-fee4-429f-8a6b-b4ab9ef7acb3"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "llama_model_loader: loaded meta data with 27 key-value pairs and 291 tensors from Models/unsloth.Q4_K_M.gguf (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
            "llama_model_loader: - kv   1:                               general.type str              = model\n",
            "llama_model_loader: - kv   2:                               general.name str              = Llama 3 8b Instruct Bnb 4bit Gaodalie...\n",
            "llama_model_loader: - kv   3:                           general.finetune str              = Instruct-bnb-4bit-gaodalie-demo\n",
            "llama_model_loader: - kv   4:                           general.basename str              = llama-3\n",
            "llama_model_loader: - kv   5:                         general.size_label str              = 8B\n",
            "llama_model_loader: - kv   6:                          llama.block_count u32              = 32\n",
            "llama_model_loader: - kv   7:                       llama.context_length u32              = 8192\n",
            "llama_model_loader: - kv   8:                     llama.embedding_length u32              = 4096\n",
            "llama_model_loader: - kv   9:                  llama.feed_forward_length u32              = 14336\n",
            "llama_model_loader: - kv  10:                 llama.attention.head_count u32              = 32\n",
            "llama_model_loader: - kv  11:              llama.attention.head_count_kv u32              = 8\n",
            "llama_model_loader: - kv  12:                       llama.rope.freq_base f32              = 500000.000000\n",
            "llama_model_loader: - kv  13:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
            "llama_model_loader: - kv  14:                          general.file_type u32              = 15\n",
            "llama_model_loader: - kv  15:                           llama.vocab_size u32              = 128256\n",
            "llama_model_loader: - kv  16:                 llama.rope.dimension_count u32              = 128\n",
            "llama_model_loader: - kv  17:                       tokenizer.ggml.model str              = gpt2\n",
            "llama_model_loader: - kv  18:                         tokenizer.ggml.pre str              = llama-bpe\n",
            "llama_model_loader: - kv  19:                      tokenizer.ggml.tokens arr[str,128256]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
            "llama_model_loader: - kv  20:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
            "llama_model_loader: - kv  21:                      tokenizer.ggml.merges arr[str,280147]  = [\"Ġ Ġ\", \"Ġ ĠĠĠ\", \"ĠĠ ĠĠ\", \"...\n",
            "llama_model_loader: - kv  22:                tokenizer.ggml.bos_token_id u32              = 128000\n",
            "llama_model_loader: - kv  23:                tokenizer.ggml.eos_token_id u32              = 128009\n",
            "llama_model_loader: - kv  24:            tokenizer.ggml.padding_token_id u32              = 128255\n",
            "llama_model_loader: - kv  25:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...\n",
            "llama_model_loader: - kv  26:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - type  f32:   65 tensors\n",
            "llama_model_loader: - type q4_K:  193 tensors\n",
            "llama_model_loader: - type q6_K:   33 tensors\n",
            "llm_load_vocab: special tokens cache size = 256\n",
            "llm_load_vocab: token to piece cache size = 0.8000 MB\n",
            "llm_load_print_meta: format           = GGUF V3 (latest)\n",
            "llm_load_print_meta: arch             = llama\n",
            "llm_load_print_meta: vocab type       = BPE\n",
            "llm_load_print_meta: n_vocab          = 128256\n",
            "llm_load_print_meta: n_merges         = 280147\n",
            "llm_load_print_meta: vocab_only       = 0\n",
            "llm_load_print_meta: n_ctx_train      = 8192\n",
            "llm_load_print_meta: n_embd           = 4096\n",
            "llm_load_print_meta: n_layer          = 32\n",
            "llm_load_print_meta: n_head           = 32\n",
            "llm_load_print_meta: n_head_kv        = 8\n",
            "llm_load_print_meta: n_rot            = 128\n",
            "llm_load_print_meta: n_swa            = 0\n",
            "llm_load_print_meta: n_embd_head_k    = 128\n",
            "llm_load_print_meta: n_embd_head_v    = 128\n",
            "llm_load_print_meta: n_gqa            = 4\n",
            "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
            "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
            "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
            "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
            "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
            "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
            "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
            "llm_load_print_meta: n_ff             = 14336\n",
            "llm_load_print_meta: n_expert         = 0\n",
            "llm_load_print_meta: n_expert_used    = 0\n",
            "llm_load_print_meta: causal attn      = 1\n",
            "llm_load_print_meta: pooling type     = 0\n",
            "llm_load_print_meta: rope type        = 0\n",
            "llm_load_print_meta: rope scaling     = linear\n",
            "llm_load_print_meta: freq_base_train  = 500000.0\n",
            "llm_load_print_meta: freq_scale_train = 1\n",
            "llm_load_print_meta: n_ctx_orig_yarn  = 8192\n",
            "llm_load_print_meta: rope_finetuned   = unknown\n",
            "llm_load_print_meta: ssm_d_conv       = 0\n",
            "llm_load_print_meta: ssm_d_inner      = 0\n",
            "llm_load_print_meta: ssm_d_state      = 0\n",
            "llm_load_print_meta: ssm_dt_rank      = 0\n",
            "llm_load_print_meta: ssm_dt_b_c_rms   = 0\n",
            "llm_load_print_meta: model type       = 8B\n",
            "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
            "llm_load_print_meta: model params     = 8.03 B\n",
            "llm_load_print_meta: model size       = 4.58 GiB (4.89 BPW) \n",
            "llm_load_print_meta: general.name     = Llama 3 8b Instruct Bnb 4bit Gaodalie Demo\n",
            "llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'\n",
            "llm_load_print_meta: EOS token        = 128009 '<|eot_id|>'\n",
            "llm_load_print_meta: PAD token        = 128255 '<|reserved_special_token_250|>'\n",
            "llm_load_print_meta: LF token         = 128 'Ä'\n",
            "llm_load_print_meta: EOT token        = 128009 '<|eot_id|>'\n",
            "llm_load_print_meta: EOG token        = 128009 '<|eot_id|>'\n",
            "llm_load_print_meta: max token length = 256\n",
            "llm_load_tensors: ggml ctx size =    0.14 MiB\n",
            "llm_load_tensors:        CPU buffer size =  4685.30 MiB\n",
            "........................................................................................\n",
            "llama_new_context_with_model: n_batch is less than GGML_KQ_MASK_PAD - increasing to 32\n",
            "llama_new_context_with_model: n_ctx      = 512\n",
            "llama_new_context_with_model: n_batch    = 32\n",
            "llama_new_context_with_model: n_ubatch   = 8\n",
            "llama_new_context_with_model: flash_attn = 0\n",
            "llama_new_context_with_model: freq_base  = 10000.0\n",
            "llama_new_context_with_model: freq_scale = 1\n",
            "llama_kv_cache_init:        CPU KV buffer size =    64.00 MiB\n",
            "llama_new_context_with_model: KV self size  =   64.00 MiB, K (f16):   32.00 MiB, V (f16):   32.00 MiB\n",
            "llama_new_context_with_model:        CPU  output buffer size =     0.49 MiB\n",
            "llama_new_context_with_model:        CPU compute buffer size =     4.04 MiB\n",
            "llama_new_context_with_model: graph nodes  = 1030\n",
            "llama_new_context_with_model: graph splits = 1\n",
            "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | RISCV_VECT = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
            "Model metadata: {'tokenizer.chat_template': \"{% set loop_messages = messages %}{% for message in loop_messages %}{% set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}{% if loop.index0 == 0 %}{% set content = bos_token + content %}{% endif %}{{ content }}{% endfor %}{% if add_generation_prompt %}{{ '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}{% endif %}\", 'tokenizer.ggml.eos_token_id': '128009', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'gpt2', 'llama.rope.dimension_count': '128', 'llama.vocab_size': '128256', 'general.file_type': '15', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'general.architecture': 'llama', 'llama.rope.freq_base': '500000.000000', 'tokenizer.ggml.padding_token_id': '128255', 'general.basename': 'llama-3', 'tokenizer.ggml.bos_token_id': '128000', 'llama.attention.head_count': '32', 'tokenizer.ggml.pre': 'llama-bpe', 'llama.context_length': '8192', 'general.name': 'Llama 3 8b Instruct Bnb 4bit Gaodalie Demo', 'general.finetune': 'Instruct-bnb-4bit-gaodalie-demo', 'general.type': 'model', 'general.size_label': '8B', 'llama.embedding_length': '4096', 'llama.feed_forward_length': '14336', 'llama.block_count': '32', 'llama.attention.head_count_kv': '8'}\n",
            "Available chat formats from metadata: chat_template.default\n",
            "Guessed chat format: llama-3\n"
          ]
        }
      ],
      "source": [
        "from langchain_community.chat_models import ChatLlamaCpp\n",
        "llm = ChatLlamaCpp(\n",
        "    model_path=Model_Path,\n",
        "    temperature=0.75,\n",
        "    max_tokens=2000,\n",
        "    context_window=2048,\n",
        "    top_p=1,\n",
        "    verbose=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "D_UlLVPfqDWP"
      },
      "outputs": [],
      "source": [
        "search = DuckDuckGoSearchResults()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "Kp-zrSSgbTap"
      },
      "outputs": [],
      "source": [
        "from langchain_core.utils.function_calling import convert_to_openai_tool\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "se-n0rcbqVDk"
      },
      "outputs": [],
      "source": [
        "# Retrieval Evaluator\n",
        "class RetrievalEvaluatorInput(BaseModel):\n",
        "    relevance_score: float = Field(..., description=\"The relevance score of the document to the query. the score should be between 0 and 1.\")\n",
        "def retrieval_evaluator(query: str, document: str) -> float:\n",
        "    prompt = PromptTemplate(\n",
        "        input_variables=[\"query\", \"document\"],\n",
        "        template=\"On a scale from 0 to 1, how relevant is the following document to the query? Query: {query}\\nDocument: {document}\\nRelevance score:\"\n",
        "    )\n",
        "    dict_schema = convert_to_openai_tool(RetrievalEvaluatorInput)\n",
        "    print(\"Prompt:\", prompt)\n",
        "    print(\"Schema:\", dict_schema)\n",
        "    chain = prompt | llm.with_structured_output(dict_schema)\n",
        "    input_variables = {\"query\": query, \"document\": document}\n",
        "    print(\"Input variables:\", input_variables)\n",
        "    result = chain.invoke(input_variables)\n",
        "    print(f\"Result from chain: {result}\")\n",
        "    relevance_score = result['relevance_score']\n",
        "    return relevance_score\n",
        "\n",
        "# Knowledge Refinement\n",
        "class KnowledgeRefinementInput(BaseModel):\n",
        "    key_points: str = Field(..., description=\"The document to extract key information from.\")\n",
        "def knowledge_refinement(document: str) -> List[str]:\n",
        "    prompt = PromptTemplate(\n",
        "        input_variables=[\"document\"],\n",
        "        template=\"Extract the key information from the following document in bullet points:\\n{document}\\nKey points:\"\n",
        "    )\n",
        "    chain = prompt | llm.with_structured_output(KnowledgeRefinementInput)\n",
        "    input_variables = {\"document\": document}\n",
        "    result = chain.invoke(input_variables).key_points\n",
        "    return [point.strip() for point in result.split('\\n') if point.strip()]\n",
        "\n",
        "# Web Search Query Rewriter\n",
        "class QueryRewriterInput(BaseModel):\n",
        "    query: str = Field(..., description=\"The query to rewrite.\")\n",
        "def rewrite_query(query: str) -> str:\n",
        "    prompt = PromptTemplate(\n",
        "        input_variables=[\"query\"],\n",
        "        template=\"Rewrite the following query to make it more suitable for a web search:\\n{query}\\nRewritten query:\"\n",
        "    )\n",
        "    chain = prompt | llm.with_structured_output(QueryRewriterInput)\n",
        "    input_variables = {\"query\": query}\n",
        "    return chain.invoke(input_variables).query.strip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "AlKwf-CiK6XV"
      },
      "outputs": [],
      "source": [
        "api_key = "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "bJMF1avMJdRC"
      },
      "outputs": [],
      "source": [
        "from langchain_core.tools import Tool\n",
        "from langchain_community.utilities import SerpAPIWrapper\n",
        "\n",
        "# Initialize the SerpAPIWrapper with your API key\n",
        "search = SerpAPIWrapper(serpapi_api_key=api_key)\n",
        "\n",
        "\n",
        "def top5_results(query: str) -> List:\n",
        "    \"\"\"\n",
        "    Retrieve the search results for a given query\n",
        "    and return them directly.\n",
        "\n",
        "    Args:\n",
        "        query (str): The search query to execute.\n",
        "\n",
        "    Returns:\n",
        "        List: The search results returned by the SerpAPIWrapper.\n",
        "    \"\"\"\n",
        "    # Use the run method and return the results directly\n",
        "    results = search.run(query)  # Pass only the query\n",
        "    print(\"Retrieved search results:\", results)  # Debugging line\n",
        "    return results  # Return the raw results\n",
        "\n",
        "# Define the tool\n",
        "tool = Tool(\n",
        "    name=\"Google Search Snippets\",\n",
        "    description=\"Search Google for recent results.\",\n",
        "    func=top5_results,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "fC3R0Kb6qZrs"
      },
      "outputs": [],
      "source": [
        "def retrieve_documents(query: str, faiss_index: FAISS, k: int = 3) -> List[str]:\n",
        "    \"\"\"\n",
        "    Retrieve documents based on a query using a FAISS index.\n",
        "\n",
        "    Args:\n",
        "        query (str): The query string to search for.\n",
        "        faiss_index (FAISS): The FAISS index used for similarity search.\n",
        "        k (int): The number of top documents to retrieve. Defaults to 3.\n",
        "\n",
        "    Returns:\n",
        "        List[str]: A list of the retrieved document contents.\n",
        "    \"\"\"\n",
        "    docs = faiss_index.similarity_search(query, k=k)\n",
        "    return [doc.page_content for doc in docs]\n",
        "\n",
        "def evaluate_documents(query: str, documents: List[str]) -> List[float]:\n",
        "    \"\"\"\n",
        "    Evaluate the relevance of documents based on a query.\n",
        "\n",
        "    Args:\n",
        "        query (str): The query string.\n",
        "        documents (List[str]): A list of document contents to evaluate.\n",
        "\n",
        "    Returns:\n",
        "        List[float]: A list of relevance scores for each document.\n",
        "    \"\"\"\n",
        "    return [retrieval_evaluator(query, doc) for doc in documents]\n",
        "\n",
        "def perform_web_search(query: str) -> Tuple[List[str], List[Tuple[str, str]]]:\n",
        "    \"\"\"\n",
        "    Perform a web search based on a query.\n",
        "    \"\"\"\n",
        "    rewritten_query = rewrite_query(query)\n",
        "    print(f\"Performing web search for query: {rewritten_query}\")  # Debugging line\n",
        "\n",
        "    try:\n",
        "        web_results = tool.run(rewritten_query)  # Only pass the rewritten query\n",
        "        print(\"Web results string:\", web_results)  # Debugging line\n",
        "        web_knowledge = knowledge_refinement(web_results)\n",
        "        print(\"Refined knowledge from web results:\", web_knowledge)  # Debugging line\n",
        "\n",
        "        return web_knowledge, web_results\n",
        "    except Exception as e:\n",
        "        print(f\"Error during web search: {e}\")\n",
        "        return [], []\n",
        "\n",
        "def generate_response(query: str, knowledge: str) -> str:\n",
        "    \"\"\"\n",
        "    Generate a response to a query using knowledge and sources.\n",
        "\n",
        "    Args:\n",
        "        query (str): The query string.\n",
        "        knowledge (str): The refined knowledge to use in the response.\n",
        "        sources (List[Tuple[str, str]]): A list of tuples containing titles and links of the sources.\n",
        "\n",
        "    Returns:\n",
        "        str: The generated response.\n",
        "    \"\"\"\n",
        "    response_prompt = PromptTemplate(\n",
        "        input_variables=[\"query\", \"knowledge\"],\n",
        "        template=\"Based on the following knowledge, answer the query. Include the sources with their links (if available) at the end of your answer:\\nQuery: {query}\\nKnowledge: {knowledge}\\nAnswer:\"\n",
        "    )\n",
        "    input_variables = {\n",
        "        \"query\": query,\n",
        "        \"knowledge\": knowledge,\n",
        "    }\n",
        "    response_chain = response_prompt | llm\n",
        "    return response_chain.invoke(input_variables).content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "M_b1oG_uqcL4"
      },
      "outputs": [],
      "source": [
        "def crag_process(query: str, faiss_index: FAISS) -> str:\n",
        "    \"\"\"\n",
        "    Process a query by retrieving, evaluating, and using documents or performing a web search to generate a response.\n",
        "\n",
        "    Args:\n",
        "        query (str): The query string to process.\n",
        "        faiss_index (FAISS): The FAISS index used for document retrieval.\n",
        "\n",
        "    Returns:\n",
        "        str: The generated response based on the query.\n",
        "    \"\"\"\n",
        "    print(f\"\\nProcessing query: {query}\")\n",
        "\n",
        "    # Retrieve and evaluate documents\n",
        "    retrieved_docs = retrieve_documents(query, faiss_index)\n",
        "    eval_scores = evaluate_documents(query, retrieved_docs)\n",
        "\n",
        "    print(f\"\\nRetrieved {len(retrieved_docs)} documents\")\n",
        "    print(f\"Evaluation scores: {eval_scores}\")\n",
        "\n",
        "    # Determine action based on evaluation scores\n",
        "    max_score = max(eval_scores)\n",
        "    sources = []\n",
        "\n",
        "    if max_score > 0.7:\n",
        "        print(\"\\nAction: Correct - Using retrieved document\")\n",
        "        best_doc = retrieved_docs[eval_scores.index(max_score)]\n",
        "        final_knowledge = best_doc\n",
        "        sources.append((\"Retrieved document\", \"\"))\n",
        "    elif max_score < 0.3:\n",
        "        print(\"\\nAction: Incorrect - Performing web search\")\n",
        "        final_knowledge, sources = perform_web_search(query)\n",
        "    else:\n",
        "        print(\"\\nAction: Ambiguous - Combining retrieved document and web search\")\n",
        "        best_doc = retrieved_docs[eval_scores.index(max_score)]\n",
        "        # Refine the retrieved knowledge\n",
        "        retrieved_knowledge = knowledge_refinement(best_doc)\n",
        "        web_knowledge, web_sources = perform_web_search(query)\n",
        "        final_knowledge = \"\\n\".join(retrieved_knowledge + web_knowledge)\n",
        "        sources = [(\"Retrieved document\", \"\")] + web_sources\n",
        "\n",
        "    print(\"\\nFinal knowledge:\")\n",
        "    print(final_knowledge)\n",
        "\n",
        "    # Generate response\n",
        "    print(\"\\nGenerating response...\")\n",
        "    response = generate_response(query, final_knowledge)\n",
        "\n",
        "    print(\"\\nResponse generated\")\n",
        "    return response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WT7iIyRzqfHt",
        "outputId": "42d17ca0-ec56-4fbf-adf8-a04e7ec15bc9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Processing query: What tells you how much of the food you should eat to get the nutrients listed on the label?\n",
            "Prompt: input_variables=['document', 'query'] input_types={} partial_variables={} template='On a scale from 0 to 1, how relevant is the following document to the query? Query: {query}\\nDocument: {document}\\nRelevance score:'\n",
            "Schema: {'type': 'function', 'function': {'name': 'RetrievalEvaluatorInput', 'description': '', 'parameters': {'type': 'object', 'properties': {'relevance_score': {'description': 'The relevance score of the document to the query. the score should be between 0 and 1.', 'type': 'number'}}, 'required': ['relevance_score']}}}\n",
            "Input variables: {'query': 'What tells you how much of the food you should eat to get the nutrients listed on the label?', 'document': \"Since it is impossible for a s cientist to record everything  that took place in an experiment, \\nfacts selected for their apparent relevance are reported . This may lead, unavoidably, to \\nproblems later if some supposedly irrelevant feature is questioned. For example, Heinrich \\nHertz  did not report the size of the room used to test Maxwell's equations, which later turned \\nout to account for a small deviation in the results. The problem is that parts of the theory \\nitself need to be ass umed in order to select and report the experimental conditions.  \\nv. Conclusion:  \\nYou have asked question s and performed an experiment to confirm your hypothesis;  your \\nconclusion is the record of the final findings in your experiment. A conclusion  is simply  a \\nsummary of the experiment. The conclusion, plain and simple, is the answer to your question \\nand it should be clear, concise and stick to the point . There are two possible outcomes to your\"}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "llama_perf_context_print:        load time =  130116.32 ms\n",
            "llama_perf_context_print: prompt eval time =       0.00 ms /   257 tokens (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:        eval time =       0.00 ms /    12 runs   (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:       total time =  140637.09 ms /   269 tokens\n",
            "Llama.generate: 49 prefix-match hit, remaining 207 prompt tokens to eval\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Result from chain: {'relevance_score': 0.1}\n",
            "Prompt: input_variables=['document', 'query'] input_types={} partial_variables={} template='On a scale from 0 to 1, how relevant is the following document to the query? Query: {query}\\nDocument: {document}\\nRelevance score:'\n",
            "Schema: {'type': 'function', 'function': {'name': 'RetrievalEvaluatorInput', 'description': '', 'parameters': {'type': 'object', 'properties': {'relevance_score': {'description': 'The relevance score of the document to the query. the score should be between 0 and 1.', 'type': 'number'}}, 'required': ['relevance_score']}}}\n",
            "Input variables: {'query': 'What tells you how much of the food you should eat to get the nutrients listed on the label?', 'document': 'summary of the experiment. The conclusion, plain and simple, is the answer to your question \\nand it should be clear, concise and stick to the point . There are two possible outcomes to your \\nexperiment: either the experiment supported the hypothesis and co nsidered true or the \\nexperiment disproved the hypothesis as false. If the hypothesis is false, the steps in the \\nscientific method is repeated to make adjustment in your tested hypothesis but if the \\nhypothesis corroborates with your conclusion then the expe riment is certified true/correct.  \\nIf the hypothesis turns out to be false, there are some questions to ask to find out why:  \\n1. What was wrong with the original hypothesis?   2. Did you make poor observations?  \\n3. Was your experiment flawed?  \\nTest Questions:  \\n1. What are the problems with the classical conception of scientific methods?  \\n2. How objective is scientific method? Can scientific method bring fourth objective \\nknowledge?'}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "llama_perf_context_print:        load time =  130116.32 ms\n",
            "llama_perf_context_print: prompt eval time =       0.00 ms /   207 tokens (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:        eval time =       0.00 ms /    12 runs   (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:       total time =  115850.97 ms /   219 tokens\n",
            "Llama.generate: 49 prefix-match hit, remaining 191 prompt tokens to eval\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Result from chain: {'relevance_score': 0.1}\n",
            "Prompt: input_variables=['document', 'query'] input_types={} partial_variables={} template='On a scale from 0 to 1, how relevant is the following document to the query? Query: {query}\\nDocument: {document}\\nRelevance score:'\n",
            "Schema: {'type': 'function', 'function': {'name': 'RetrievalEvaluatorInput', 'description': '', 'parameters': {'type': 'object', 'properties': {'relevance_score': {'description': 'The relevance score of the document to the query. the score should be between 0 and 1.', 'type': 'number'}}, 'required': ['relevance_score']}}}\n",
            "Input variables: {'query': 'What tells you how much of the food you should eat to get the nutrients listed on the label?', 'document': 'vary the conditions for each me asurement; to help isolate what has changed. Depending on \\nthe predictions, the experiments can have different shapes. It could be a classical experiment \\nin a laboratory setting, a  double -blind  study or an archaeological excavation .  \\nScientists assume an attitude of openness and accountability on the part of those conducting \\nan exp eriment. Detailed record keeping is essential, to aid in recording and reporting on the \\nexperimental results, and providing evidence of the effectiveness and integrity of the \\nprocedure. They will also assist in reproducing the experimental results.  \\n\\uf0b7 Experi ment and its Problematic  \\nAt any stage of experimentation, it is possible to refine its accuracy and precision  so that \\nsome considerations may lead the scientist to repeat an earlier part o f the process. Failure to \\ndevelop an interesting hypothesis may lead a scientist to re -define the subject they are'}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "llama_perf_context_print:        load time =  130116.32 ms\n",
            "llama_perf_context_print: prompt eval time =       0.00 ms /   191 tokens (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:        eval time =       0.00 ms /    12 runs   (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:       total time =  115361.16 ms /   203 tokens\n",
            "Llama.generate: 5 prefix-match hit, remaining 44 prompt tokens to eval\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Result from chain: {'relevance_score': 0.05}\n",
            "\n",
            "Retrieved 3 documents\n",
            "Evaluation scores: [0.1, 0.1, 0.05]\n",
            "\n",
            "Action: Incorrect - Performing web search\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "llama_perf_context_print:        load time =  130116.32 ms\n",
            "llama_perf_context_print: prompt eval time =       0.00 ms /    44 tokens (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:        eval time =       0.00 ms /    18 runs   (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:       total time =   36890.41 ms /    62 tokens\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Performing web search for query: How to determine serving size for optimal nutrient intake from food labels?\n",
            "Retrieved search results: First, look at the serving size and the number of servings per container, which are at the top of the label. The serving size is shown as a common household measure that is appropriate to the food (such as cup, tablespoon, piece, slice, or jar), followed by the metric amount in grams (g).\n",
            "Web results string: First, look at the serving size and the number of servings per container, which are at the top of the label. The serving size is shown as a common household measure that is appropriate to the food (such as cup, tablespoon, piece, slice, or jar), followed by the metric amount in grams (g).\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Llama.generate: 5 prefix-match hit, remaining 84 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =  130116.32 ms\n",
            "llama_perf_context_print: prompt eval time =       0.00 ms /    84 tokens (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:        eval time =       0.00 ms /    24 runs   (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:       total time =   62802.27 ms /   108 tokens\n",
            "Llama.generate: 5 prefix-match hit, remaining 77 prompt tokens to eval\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Refined knowledge from web results: ['• Serving size and number of servings per container are at the top of the label.']\n",
            "\n",
            "Final knowledge:\n",
            "['• Serving size and number of servings per container are at the top of the label.']\n",
            "\n",
            "Generating response...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "llama_perf_context_print:        load time =  130116.32 ms\n",
            "llama_perf_context_print: prompt eval time =       0.00 ms /    77 tokens (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:        eval time =       0.00 ms /    52 runs   (    0.00 ms per token,      inf tokens per second)\n",
            "llama_perf_context_print:       total time =   86933.83 ms /   129 tokens\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Response generated\n",
            "Query: What tells you how much of the food you should eat to get the nutrients listed on the label?\n",
            "Answer: According to the provided knowledge, the serving size and number of servings per container are at the top of the label. This information tells you how much of the food you should eat to get the nutrients listed on the label.\n",
            "\n",
            "Source: [No specific source available]\n"
          ]
        }
      ],
      "source": [
        "query = \"What tells you how much of the food you should eat to get the nutrients listed on the label?\"\n",
        "result = crag_process(query, vectorstore)\n",
        "print(f\"Query: {query}\")\n",
        "print(f\"Answer: {result}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "10a9fb6d877043d0b38d5afbce0bb2f9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b60432cf141c472e96f954c08fe8eac0",
              "IPY_MODEL_4a32671c2a364cef94a94fa4fcd3efab",
              "IPY_MODEL_26ce532afb7f4416be1179f00e266372"
            ],
            "layout": "IPY_MODEL_1ef3f5d94b5f440295a97358583cf8a6"
          }
        },
        "186e9cb83b11466b86d339e602df925d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1ef3f5d94b5f440295a97358583cf8a6": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "26ce532afb7f4416be1179f00e266372": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_da9a87c403144c0485ccf8c254aeaf18",
            "placeholder": "​",
            "style": "IPY_MODEL_b0020f10ba7d4346813f69fc74e9c696",
            "value": " 4.92G/4.92G [01:01&lt;00:00, 82.7MB/s]"
          }
        },
        "4a32671c2a364cef94a94fa4fcd3efab": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bad5edf81fcc4490a7444a3589185a49",
            "max": 4920734240,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_725681421d554b8a9bea621c4e06ec92",
            "value": 4920734240
          }
        },
        "725681421d554b8a9bea621c4e06ec92": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "982afd866ab64066b41e3e2ae84ba55a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b0020f10ba7d4346813f69fc74e9c696": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b60432cf141c472e96f954c08fe8eac0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_186e9cb83b11466b86d339e602df925d",
            "placeholder": "​",
            "style": "IPY_MODEL_982afd866ab64066b41e3e2ae84ba55a",
            "value": "unsloth.Q4_K_M.gguf: 100%"
          }
        },
        "bad5edf81fcc4490a7444a3589185a49": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "da9a87c403144c0485ccf8c254aeaf18": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
